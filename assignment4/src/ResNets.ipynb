{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, warnings, torchvision, os, h5py, time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, sampler, SubsetRandomSampler, TensorDataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if cuda is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on Tesla T4 Device {}'.format(str(torch.cuda.current_device())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening h5 files\n",
    "ROOT_DIR = \"/home/nikunjlad\"\n",
    "hf = h5py.File(os.path.join(ROOT_DIR,\"data/cifar-10/cifar10.h5\"), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test data with labels being converted to numpy array from HDF5 format\n",
    "x_train = np.array(hf.get(\"X_train\"), dtype=np.float32) \n",
    "y_train = np.array(hf.get(\"y_train\"), dtype=np.int64)\n",
    "x_test = np.array(hf.get(\"X_test\"), dtype=np.float32)\n",
    "y_test = np.array(hf.get(\"y_test\"), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data: \", x_train.shape)\n",
    "print(\"Training labels: \", y_train.shape)\n",
    "print(\"Testing data: \", x_test.shape)\n",
    "print(\"Testing labels: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.10\n",
    "num_train = len(x_train)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "X_train_1 = x_train[train_idx,:,:,:]\n",
    "y_train_1 = y_train[train_idx]\n",
    "X_valid_1 = x_train[valid_idx, :, :, :]\n",
    "y_valid_1 = y_train[valid_idx]\n",
    "train_data =list(X_train_1.transpose(0, 3, 1, 2)) \n",
    "train_targets = list(y_train_1)\n",
    "valid_data =list(X_valid_1.transpose(0, 3, 1, 2)) \n",
    "valid_targets = list(y_valid_1)\n",
    "test_data = list(x_test.transpose(0, 3, 1, 2))\n",
    "test_targets = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = Dataset(train_data, train_targets, transform_train)\n",
    "valid_dataset = Dataset(valid_data, valid_targets, transform = transform_test)\n",
    "test_dataset = Dataset(test_data, test_targets, transform = transform_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = len(train_dataset)\n",
    "valid_data_size = len(valid_dataset)\n",
    "test_data_size = len(test_dataset)\n",
    "num_train_data_batches = len(train_dataloader)\n",
    "num_valid_data_batches = len(valid_dataloader)\n",
    "num_test_data_batches = len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training samples: \", train_data_size)\n",
    "print(\"{} batches each having 64 samples\".format(str(num_train_data_batches)))\n",
    "print(\"Number of validation samples: \", valid_data_size)\n",
    "print(\"{} batches each having 64 samples\".format(str(num_valid_data_batches)))\n",
    "print(\"Number of testing samples: \", test_data_size)\n",
    "print(\"{} batches each having 64 samples\".format(str(num_test_data_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "images, labels = batch\n",
    "\n",
    "grid = torchvision.utils.make_grid(images[:64], nrow=8)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "\n",
    "for data, target in train_dataloader:\n",
    "    print(\"Batch image tensor dimensions: \", data.shape)\n",
    "    print(\"Batch label tensor dimensions: \", target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "net = ResNet34()\n",
    "net = net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.current_device() in range(torch.cuda.device_count()):\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = list()\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "     \n",
    "    # Set to training mode\n",
    "    net.train()\n",
    "     \n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    " \n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    " \n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "         \n",
    "        # Clean existing gradients\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = net(inputs)\n",
    "         \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "         \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "         \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "         \n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "         \n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "         \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "         \n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "         \n",
    "        print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item() * 100))\n",
    "        \n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    " \n",
    "        # Set to evaluation mode\n",
    "        net.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        for j, (inputs, labels) in enumerate(valid_dataloader):\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item() * 100))\n",
    "     \n",
    "    # Find average training loss and training accuracy\n",
    "    avg_train_loss = train_loss/train_data_size \n",
    "    avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "    # Find average training loss and training accuracy\n",
    "    avg_valid_loss = valid_loss/valid_data_size \n",
    "    avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "\n",
    "    history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(\"Epoch : {:03d}, Training: Loss: {:.4f}, \\\n",
    "            Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, \\\n",
    "            Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, \\\n",
    "                                                     avg_train_acc*100, avg_valid_loss, \\\n",
    "                                                     avg_valid_acc*100, epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Validation - No gradient tracking needed\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    for j, (inputs, labels) in enumerate(test_dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "        print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item() * 100))\n",
    "\n",
    "    \n",
    "    avg_test_loss = test_loss/test_data_size \n",
    "    avg_test_acc = test_acc/float(test_data_size)\n",
    "    \n",
    "    print(\"Test: Loss : {:.4f}, Accuracy: {:.4f}%\".format(avg_test_loss, avg_test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Run1\n",
    "1. Architecture = ResNet-18\n",
    "2. Epochs = 100\n",
    "3. batch size = 64\n",
    "4. optimizer = SGD\n",
    "5. alpha = 0.0005\n",
    "6. training loss = 0.0396\n",
    "7. training accuracy = 98.7225%\n",
    "8. validation loss = 0.3397\n",
    "9. validation accuracy = 91.4%\n",
    "10. approx runtime = ~101 minutes (1.65hrs) / 61secs for 1 epoch\n",
    "11. test loss = 3688\n",
    "12. test accuracy = 91.08%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
